import argparse
import json
import shutil
import sys

from pathlib import Path

# Add the src directory to the path to allow for module imports
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from cognitive.persistent_recursion import run_analysis
from cognitive.interactive_approval import (
    InteractiveApprovalSystem, FixProposal, FixSeverity, ApprovalDecision
)
from cognitive.enhanced_patterns.memory_enhanced_false_positive_detector import MemoryEnhancedFalsePositiveDetector
from cognitive.enhanced_patterns.context_analyzer import ContextAnalyzer
from cognitive.enhanced_patterns.memory_pruning_system import MemoryPruningSystem, PruningStrategy
from cognitive.enhanced_patterns.improvement_cycle_tracker import ImprovementCycleTracker
from cognitive.memory.memory.engine import MemoryEngine

def run_fixer(project_path: str, issues_file: str):
    """Interactively fixes issues in a project."""
    print(f"ğŸŒ€ Enhanced PRI: Interactive Fix Application")
    print(f"ğŸ“ Project: {project_path}")
    print(f"ğŸ¯ Mode: Automatic")
    print(f"ğŸ›¡ï¸  Auto-approve safe fixes: Yes")

    try:
        with open(issues_file, 'r') as f:
            issues = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error loading issues file: {e}")
        return

    approval_system = InteractiveApprovalSystem(auto_approve_safe=True, interactive_mode=False)
    fix_proposals = convert_issues_to_proposals(issues)
    approved_fixes, rejected_fixes = approval_system.process_fix_batch(fix_proposals)

    if approved_fixes:
        print("\nğŸ”§ Applying approved fixes...")
        for fix in approved_fixes:
            apply_fix(project_path, fix)
        print("Fix application complete.")
    else:
        print("\nNo fixes were approved.")

def apply_fix(project_path, fix):
    """Applies a single fix to a file."""
    try:
        file_path = Path(project_path) / fix.file_path
        if not file_path.exists():
            print(f"  âŒ Error: File not found at {file_path}")
            return

        # Create a backup
        backup_path = file_path.with_suffix(file_path.suffix + '.bak')
        shutil.copy2(file_path, backup_path)

        with open(file_path, 'r+') as f:
            lines = f.readlines()
            # This is a simplified approach, a more robust solution would be needed for multi-line fixes
            if fix.line_number - 1 < len(lines):
                lines[fix.line_number - 1] = lines[fix.line_number - 1].replace(fix.original_code, fix.proposed_fix)
                f.seek(0)
                f.writelines(lines)
                f.truncate()
                print(f"  âœ… Applied fix: {fix.issue_type} in {fix.file_path}")
            else:
                print(f"  âŒ Error: Line number {fix.line_number} is out of bounds for file {fix.file_path}")

    except Exception as e:
        print(f"  âŒ Error applying fix to {fix.file_path}: {e}")

def convert_issues_to_proposals(issues):
    """Converts a list of issues to a list of FixProposal objects."""
    proposals = []
    for issue in issues:
        severity_map = {
            'critical': FixSeverity.CRITICAL,
            'high': FixSeverity.HIGH,
            'medium': FixSeverity.MEDIUM,
            'low': FixSeverity.LOW,
            'cosmetic': FixSeverity.COSMETIC
        }
        proposal = FixProposal(
            file_path=issue.get('file_path'),
            issue_type=issue.get('type'),
            severity=severity_map.get(issue.get('severity'), FixSeverity.MEDIUM),
            description=issue.get('description'),
            original_code=issue.get('original_code'),
            proposed_fix=issue.get('suggested_fix'),
            line_number=issue.get('line'),
            educational_explanation=f"This fix addresses a {issue.get('issue_type')} issue.",
            safety_score=issue.get('safety_score', 50),
            context=issue.get('context', 'unknown'),
            auto_approvable=True
        )
        proposals.append(proposal)
    return proposals

def run_training(issues_file: str, interactive: bool, batch_file: str = None):
    """Train PRI by flagging false positives."""
    print(f"ğŸ§  PRI Training Mode: False Positive Detection")
    print(f"ğŸ“ Issues file: {issues_file}")
    print(f"ğŸ¯ Mode: {'Interactive' if interactive else 'Batch'}")

    try:
        # Load issues
        with open(issues_file, 'r') as f:
            issues = json.load(f)

        # Initialize memory-enhanced detector
        memory_engine = MemoryEngine()
        context_analyzer = ContextAnalyzer()
        fp_detector = MemoryEnhancedFalsePositiveDetector(memory_engine, context_analyzer)

        if interactive:
            run_interactive_training(issues, fp_detector)
        elif batch_file:
            run_batch_training(issues, batch_file, fp_detector)
        else:
            print("âŒ Error: Must specify either --interactive or --batch-file")

    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âŒ Error loading files: {e}")
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")

def run_interactive_training(issues: list, fp_detector):
    """Interactive training mode for flagging false positives."""
    print(f"\nğŸ¯ Interactive Training Session")
    print(f"ğŸ“Š Total issues to review: {len(issues)}")
    print("ğŸ’¡ Commands: (y)es = false positive, (n)o = valid issue, (s)kip, (q)uit")
    print()

    trained_count = 0
    skipped_count = 0

    for i, issue in enumerate(issues, 1):
        print(f"ğŸ“‹ Issue {i}/{len(issues)}")
        print(f"   Type: {issue.get('type', 'unknown')}")
        print(f"   Severity: {issue.get('severity', 'unknown')}")
        print(f"   Description: {issue.get('description', 'No description')}")
        print(f"   Context: {issue.get('context', 'unknown')}")

        if issue.get('suggestion'):
            print(f"   Suggestion: {issue.get('suggestion')}")

        print()

        while True:
            response = input("Is this a false positive? (y/n/s/q): ").lower().strip()

            if response == 'q':
                print(f"\nâœ… Training session completed.")
                print(f"ğŸ“Š Trained on {trained_count} issues, skipped {skipped_count}")
                return

            elif response == 's':
                skipped_count += 1
                print("â­ï¸  Skipped")
                break

            elif response in ['y', 'n']:
                is_false_positive = (response == 'y')

                # Get user reasoning
                reasoning = input("Brief reason (optional): ").strip()
                if not reasoning:
                    reasoning = "User classification during training"

                # Store the training data
                try:
                    # For training, we'll use a dummy file path based on context
                    file_path = f"training_data/{issue.get('context', 'unknown')}_file.py"

                    # Use asyncio to run the async method
                    import asyncio
                    asyncio.run(fp_detector.learn_from_user_feedback(
                        issue, file_path, is_false_positive, reasoning, confidence=1.0
                    ))

                    trained_count += 1
                    status = "false positive" if is_false_positive else "valid issue"
                    print(f"âœ… Learned: {status}")

                except Exception as e:
                    print(f"âš ï¸  Error storing feedback: {e}")

                break

            else:
                print("Invalid response. Use y/n/s/q")

        print("-" * 50)

    print(f"\nğŸ‰ Training completed!")
    print(f"ğŸ“Š Final stats: {trained_count} trained, {skipped_count} skipped")

def run_batch_training(issues: list, batch_file: str, fp_detector):
    """Batch training mode using pre-labeled data."""
    print(f"\nğŸ“¦ Batch Training Mode")
    print(f"ğŸ“ Batch file: {batch_file}")

    try:
        with open(batch_file, 'r') as f:
            batch_data = json.load(f)

        print(f"ğŸ“Š Found {len(batch_data)} labeled examples")

        trained_count = 0
        errors = 0

        for label_data in batch_data:
            try:
                # Find matching issue
                issue_id = label_data.get('issue_id')
                matching_issue = None

                for issue in issues:
                    if (issue.get('line') == issue_id or
                        issue.get('description') == label_data.get('description')):
                        matching_issue = issue
                        break

                if not matching_issue:
                    print(f"âš ï¸  Could not find matching issue for: {label_data.get('description', 'unknown')}")
                    continue

                # Apply the label
                is_false_positive = label_data.get('is_false_positive', False)
                reasoning = label_data.get('reasoning', 'Batch training data')
                file_path = label_data.get('file_path', 'batch_training_file.py')

                asyncio.run(fp_detector.learn_from_user_feedback(
                    matching_issue, file_path, is_false_positive, reasoning, confidence=0.9
                ))

                trained_count += 1

            except Exception as e:
                print(f"âŒ Error processing batch item: {e}")
                errors += 1

        print(f"\nâœ… Batch training completed!")
        print(f"ğŸ“Š Successfully trained: {trained_count}")
        print(f"âŒ Errors: {errors}")

    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âŒ Error loading batch file: {e}")

def run_stats(detailed: bool):
    """Show false positive detection statistics."""
    print("ğŸ“Š PRI False Positive Detection Statistics")
    print("=" * 50)

    try:
        # Initialize detector to get stats
        memory_engine = MemoryEngine()
        context_analyzer = ContextAnalyzer()
        fp_detector = MemoryEnhancedFalsePositiveDetector(memory_engine, context_analyzer)

        stats = asyncio.run(fp_detector.get_false_positive_statistics())

        if "error" in stats:
            print(f"âŒ Error getting statistics: {stats['error']}")
            return

        if "message" in stats:
            print(f"ğŸ’¡ {stats['message']}")
            return

        # Display basic statistics
        print(f"ğŸ”¢ Total Analyses: {stats['total_analyses']}")
        print(f"ğŸš« False Positives: {stats['total_false_positives']}")
        print(f"ğŸ“ˆ False Positive Rate: {stats['overall_fp_rate']:.2%}")
        print(f"ğŸ‘¤ User Feedbacks: {stats['user_feedbacks']}")
        print(f"ğŸ§  Learning Effectiveness: {stats['learning_effectiveness']:.2%}")

        if detailed and stats.get('issue_type_statistics'):
            print(f"\nğŸ“‹ Issue Type Breakdown:")
            print("-" * 30)

            for issue_type, type_stats in stats['issue_type_statistics'].items():
                print(f"  {issue_type}:")
                print(f"    Total: {type_stats['total']}")
                print(f"    False Positives: {type_stats['false_positives']}")
                print(f"    FP Rate: {type_stats['fp_rate']:.2%}")
                print()

        if detailed and stats.get('context_statistics'):
            print(f"ğŸ“ Context Breakdown:")
            print("-" * 30)

            for context, context_stats in stats['context_statistics'].items():
                print(f"  {context}:")
                print(f"    Total: {context_stats['total']}")
                print(f"    False Positives: {context_stats['false_positives']}")
                print(f"    FP Rate: {context_stats['fp_rate']:.2%}")
                print()

        print("ğŸ’¡ Use 'python mesopredator_cli.py train --interactive --issues-file <file>' to improve detection")

    except Exception as e:
        print(f"âŒ Error getting statistics: {e}")
        import traceback
        print(traceback.format_exc())

def run_pruning(strategy: str, dry_run: bool, namespace: str = None, aggressive: bool = False):
    """Run memory pruning with the specified strategy."""
    print("ğŸ§¹ PRI Memory Pruning")
    print("=" * 40)
    print(f"ğŸ¯ Strategy: {strategy}")
    print(f"ğŸ” Mode: {'Dry run (preview only)' if dry_run else 'Live pruning'}")

    if namespace:
        print(f"ğŸ“‚ Target namespace: {namespace}")
    else:
        print("ğŸ“‚ Target: All namespaces")

    if aggressive:
        print("âš¡ Aggressive pruning: Enabled")

    try:
        # Initialize pruning system
        memory_engine = MemoryEngine()
        pruning_system = MemoryPruningSystem(memory_engine)

        # Convert strategy string to enum
        strategy_map = {
            'age_based': PruningStrategy.AGE_BASED,
            'redundancy_based': PruningStrategy.REDUNDANCY_BASED,
            'quality_based': PruningStrategy.QUALITY_BASED,
            'hybrid': PruningStrategy.HYBRID
        }

        strategy_enum = strategy_map.get(strategy, PruningStrategy.HYBRID)

        if dry_run:
            print("\nğŸ” DRY RUN - Analyzing what would be pruned...")
            # For dry run, we'd implement a preview mode
            print("ğŸ’¡ Dry run mode not yet implemented - use with caution!")
            return

        print(f"\nğŸš€ Starting pruning with {strategy} strategy...")


        if namespace:
            # Prune specific namespace
            result = asyncio.run(pruning_system._prune_namespace(namespace, strategy_enum))
            print(f"\nâœ… Namespace pruning completed!")
            print(f"ğŸ“Š Results for {namespace}:")
            print(f"   ğŸ—‘ï¸  Removed: {result['memories_removed']}")
            print(f"   ğŸ“¦ Consolidated: {result['memories_consolidated']}")
            print(f"   ğŸ“ˆ Initial count: {result['initial_count']}")
            print(f"   ğŸ“‰ Final count: {result['final_count']}")
        else:
            # Prune all namespaces
            result = asyncio.run(pruning_system.prune_all_namespaces(strategy_enum))
            print(f"\nâœ… Global pruning completed!")
            print(f"ğŸ“Š Overall Results:")
            print(f"   ğŸ—‘ï¸  Total removed: {result.memories_removed}")
            print(f"   ğŸ“¦ Total consolidated: {result.memories_consolidated}")
            print(f"   ğŸ“ˆ Before: {result.total_memories_before} memories")
            print(f"   ğŸ“‰ After: {result.total_memories_after} memories")
            print(f"   ğŸ’¾ Space saved: {result.space_saved_mb:.2f} MB")
            print(f"   â±ï¸  Time taken: {result.pruning_time_seconds:.2f} seconds")

            if result.namespace_results:
                print(f"\nğŸ“‚ Namespace breakdown:")
                for ns, ns_result in result.namespace_results.items():
                    if isinstance(ns_result, dict) and 'memories_removed' in ns_result:
                        print(f"   {ns}: -{ns_result['memories_removed']}, consolidated: {ns_result['memories_consolidated']}")

        print(f"\nğŸ’¡ Tip: Use 'python mesopredator_cli.py stats --detailed' to see updated memory statistics")

    except Exception as e:
        print(f"âŒ Error during pruning: {e}")
        print(traceback.format_exc())

def run_cycle_tracking(command: str, issues_file: str = None, project_path: str = None, previous_issues_file: str = None):
    """Run improvement cycle tracking operations."""
    print("ğŸ”„ PRI Improvement Cycle Tracking")
    print("=" * 45)

    try:
        # Initialize cycle tracker
        memory_engine = MemoryEngine()
        cycle_tracker = ImprovementCycleTracker(memory_engine)


        if command == "manual_fixes":
            if not issues_file or not project_path:
                print("âŒ Error: --issues-file and --project-path required for manual fixes detection")
                return

            print(f"ğŸ” Detecting manual fixes in: {project_path}")
            print(f"ğŸ“ Current scan file: {issues_file}")

            # Load current issues
            with open(issues_file, 'r') as f:
                current_issues = json.load(f)

            # Detect manual fixes
            manual_fixes = asyncio.run(cycle_tracker.detect_manual_fixes_in_scan(current_issues, project_path))

            print(f"\nğŸ› ï¸  Manual Fixes Detected: {len(manual_fixes)}")

            if manual_fixes:
                print("\nğŸ“‹ Manual Fix Details:")
                for fix in manual_fixes:
                    print(f"   â€¢ {fix['issue_type']} in {fix['file_path']}")
                    print(f"     Context: {fix['file_context']}")
                    print(f"     Detected: {fix['manual_fix_detected_date']}")
                    print()
            else:
                print("   No manual fixes detected in recent cycles")

        elif command == "scan_comparison":
            if not issues_file or not project_path or not previous_issues_file:
                print("âŒ Error: --issues-file, --previous-issues-file, and --project-path required")
                return

            print(f"ğŸ“Š Comparing scans for: {project_path}")
            print(f"ğŸ“ Previous scan: {previous_issues_file}")
            print(f"ğŸ“ Current scan: {issues_file}")

            # Load issues
            with open(previous_issues_file, 'r') as f:
                previous_issues = json.load(f)
            with open(issues_file, 'r') as f:
                current_issues = json.load(f)

            # Track comparison metrics
            metrics = asyncio.run(cycle_tracker.track_scan_comparison_metrics(
                previous_issues, current_issues, project_path
            ))

            if metrics:
                print(f"\nğŸ“ˆ Scan Comparison Results:")
                print(f"   ğŸ“‹ Previous issues: {metrics['previous_issues_count']}")
                print(f"   ğŸ“‹ Current issues: {metrics['current_issues_count']}")
                print(f"   âœ… Total resolved: {metrics['total_resolved']}")
                print(f"   ğŸ› ï¸  Manual fixes: {metrics['manual_fixes_detected']}")
                print(f"   ğŸ¤– Automated fixes: {metrics['automated_fixes_estimated']}")
                print(f"   ğŸ“Š Manual fix rate: {metrics['manual_fix_rate']:.1%}")
                print(f"   ğŸ“Š Automated fix rate: {metrics['automated_fix_rate']:.1%}")

                if metrics['manual_fix_types_breakdown']:
                    print(f"\nğŸ”§ Manual Fix Types:")
                    for fix_type, count in metrics['manual_fix_types_breakdown'].items():
                        print(f"   â€¢ {fix_type}: {count}")

        elif command == "patterns":
            print("ğŸ” Analyzing manual fix patterns...")

            patterns = asyncio.run(cycle_tracker.get_manual_fix_patterns())

            if "error" in patterns:
                print(f"âŒ Error: {patterns['error']}")
                return

            if "message" in patterns:
                print(f"ğŸ’¡ {patterns['message']}")
                return

            print(f"\nğŸ“Š Manual Fix Pattern Analysis:")
            print(f"   ğŸ”¢ Total manual fixes: {patterns['total_manual_fixes']}")

            if patterns['issue_types']:
                print(f"\nğŸ› Issue Types Manually Fixed:")
                for issue_type, data in patterns['issue_types'].items():
                    print(f"   â€¢ {issue_type}: {data['count']} fixes")
                    print(f"     Contexts: {', '.join(data['contexts'])}")

            if patterns['file_contexts']:
                print(f"\nğŸ“ File Contexts:")
                for context, count in patterns['file_contexts'].items():
                    print(f"   â€¢ {context}: {count} fixes")

            if patterns.get('average_manual_fix_time_hours'):
                print(f"\nâ±ï¸  Timing Analysis:")
                print(f"   â€¢ Average fix time: {patterns['average_manual_fix_time_hours']:.1f} hours")
                print(f"   â€¢ Median fix time: {patterns['median_manual_fix_time_hours']:.1f} hours")

            if patterns['automation_opportunities']:
                print(f"\nğŸ¤– Automation Opportunities:")
                for opp in patterns['automation_opportunities']:
                    print(f"   â€¢ {opp['issue_type']} ({opp['frequency']} occurrences)")
                    print(f"     Potential: {opp['automation_potential']}")
                    print(f"     Recommendation: {opp['recommendation']}")
                    print()

        elif command == "cycle_metrics":
            print("ğŸ“Š Analyzing improvement cycle patterns...")

            cycle_metrics = asyncio.run(cycle_tracker.analyze_cycle_patterns())

            print(f"\nğŸ”„ Cycle Metrics:")
            print(f"   ğŸ“Š Total cycles: {cycle_metrics.total_cycles}")
            print(f"   âœ… Completed: {cycle_metrics.completed_cycles}")
            print(f"   ğŸ“ˆ Success rate: {cycle_metrics.success_rate:.1%}")
            print(f"   â±ï¸  Average cycle time: {cycle_metrics.average_cycle_time_hours:.1f} hours")
            print(f"   ğŸš€ Learning velocity: {cycle_metrics.learning_velocity:.2f} cycles/day")
            print(f"   ğŸ”§ Fix application rate: {cycle_metrics.fix_application_rate:.1%}")

            if cycle_metrics.common_failure_points:
                print(f"\nâš ï¸  Common Failure Points:")
                for stage, count in cycle_metrics.common_failure_points.items():
                    print(f"   â€¢ {stage}: {count} failures")

            if cycle_metrics.pattern_effectiveness_by_type:
                print(f"\nğŸ¯ Pattern Effectiveness:")
                for pattern, effectiveness in cycle_metrics.pattern_effectiveness_by_type.items():
                    print(f"   â€¢ {pattern}: {effectiveness:.1%}")

        else:
            print(f"âŒ Unknown cycle tracking command: {command}")
            print("ğŸ’¡ Available commands: manual_fixes, scan_comparison, patterns, cycle_metrics")

    except Exception as e:
        print(f"âŒ Error in cycle tracking: {e}")
        print(traceback.format_exc())

def main():
    """Main entry point for the PRI CLI tool."""
    parser = argparse.ArgumentParser(
        description="Persistent Recursive Intelligence (PRI) CLI Tool",
        epilog="A tool for analyzing and improving codebases with self-evolving AI."
    )
    subparsers = parser.add_subparsers(dest='command', required=True, help='Available commands')

    # --- Analyze Command ---
    parser_analyze = subparsers.add_parser('analyze', help='Analyze a project and store insights.')
    parser_analyze.add_argument('project_path', type=str, help='The path to the project directory to analyze.')
    parser_analyze.add_argument('--output-file', type=str, help='Path to save the analysis results as a JSON file.')

    # --- Fix Command ---
    parser_fix = subparsers.add_parser('fix', help='Interactively fix issues in a project.')
    parser_fix.add_argument('project_path', type=str, help='The path to the project directory to fix.')
    parser_fix.add_argument('--issues-file', type=str, required=True, help='JSON file containing the list of issues to address.')

    # --- Train Command ---
    parser_train = subparsers.add_parser('train', help='Train PRI by flagging false positives.')
    parser_train.add_argument('--issues-file', type=str, required=True, help='JSON file containing analysis results.')
    parser_train.add_argument('--interactive', action='store_true', help='Interactive mode for reviewing and flagging issues.')
    parser_train.add_argument('--batch-file', type=str, help='Batch file with pre-labeled false positives.')

    # --- Stats Command ---
    parser_stats = subparsers.add_parser('stats', help='Show false positive detection statistics.')
    parser_stats.add_argument('--detailed', action='store_true', help='Show detailed statistics by issue type and context.')

    # --- Prune Command ---
    parser_prune = subparsers.add_parser('prune', help='Prune memory system to remove redundant patterns.')
    parser_prune.add_argument('--strategy', choices=['age_based', 'redundancy_based', 'quality_based', 'hybrid'],
                             default='hybrid', help='Pruning strategy to use.')
    parser_prune.add_argument('--dry-run', action='store_true', help='Show what would be pruned without actually doing it.')
    parser_prune.add_argument('--namespace', type=str, help='Prune specific namespace only.')
    parser_prune.add_argument('--aggressive', action='store_true', help='Use aggressive pruning for over-represented patterns.')

    # --- Cycle Tracking Command ---
    parser_cycle = subparsers.add_parser('cycle', help='Track improvement cycles and detect manual fixes.')
    parser_cycle.add_argument('cycle_command', choices=['manual_fixes', 'scan_comparison', 'patterns', 'cycle_metrics'],
                             help='Cycle tracking operation to perform.')
    parser_cycle.add_argument('--issues-file', type=str, help='JSON file with current scan issues.')
    parser_cycle.add_argument('--previous-issues-file', type=str, help='JSON file with previous scan issues (for comparison).')
    parser_cycle.add_argument('--project-path', type=str, help='Path to the project being analyzed.')

    args = parser.parse_args()

    if args.command == 'analyze':
        issues = run_analysis(args.project_path)
        if args.output_file:
            with open(args.output_file, 'w') as f:
                json.dump(issues, f, indent=4)
            print(f"ğŸ’¾ Analysis results saved to {args.output_file}")

    elif args.command == 'fix':
        run_fixer(args.project_path, args.issues_file)

    elif args.command == 'train':
        run_training(args.issues_file, args.interactive, args.batch_file)

    elif args.command == 'stats':
        run_stats(args.detailed)

    elif args.command == 'prune':
        run_pruning(args.strategy, args.dry_run, args.namespace, args.aggressive)

    elif args.command == 'cycle':
        run_cycle_tracking(args.cycle_command, args.issues_file, args.project_path, args.previous_issues_file)

if __name__ == '__main__':
    main()